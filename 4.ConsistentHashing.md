# Consistent Hashing

While designing a scalable system, the most important aspect is defining how the data will be partitioned and replicated across servers. Letâ€™s first define these terms before moving on:

**Data partitioning**: It is the process of distributing data across a set of servers. It improves the scalability and performance of the system.

**Data replication**: It is the process of making multiple copies of data and storing them on different servers. It improves the availability and durability of the data across the system

#### There are two challenges when we try to distribute data:

âœ“ How do we know on which node a particular piece of data will be stored?

âœ“ When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes? Additionally, how can we minimize data movement when nodes join or leave?

Solution:
A naive approach will use a suitable hash function to map the data key to a number. 
Then, find the server by applying modulo on this number and the total number of servers. For example:

![image](https://user-images.githubusercontent.com/33947539/146911392-8495445e-5e25-4b7e-b975-a2d07b9a2437.png)

### Problem that Consistent Hashing solves:
The scheme described in the above diagram solves the problem of finding a server for storing/retrieving the data. But when we add or remove a server, all our existing mappings will be broken. This is because the total number of servers will be changed, which was used to find the actual server storing the data. So to get things working again, we have to remap all the keys and move our data based on the new server count, which will be a complete mess!

## Consistent Hashing to the rescue:

ðŸ‘‰ ***Consistent Hashing maps data to physical nodes and ensures that only a small set of keys move when servers are added or removed.***

![image](https://user-images.githubusercontent.com/33947539/146912489-455c2916-7470-488f-9877-ca0cad45cd0f.png)

- Consistent Hashing stores the data managed by a distributed system in a ring
- With consistent hashing, the ring is divided into smaller, predefined ranges. Each node is assigned one of these ranges. The start of the range is called a **token**.
- The range assigned to each node is computed as follows:

   Range start:  Token value
   Range end:    Next token value - 1

- Whenever the system needs to read or write data, the first step it performs is to apply the MD5 hashing algorithm to the key. The output of this hashing algorithm determines within which range the data lies and hence, on which node the data will be stored.

![image](https://user-images.githubusercontent.com/33947539/146913172-ed8c0744-1066-4600-aa7e-a90f44b2ae63.png)

## Virtual nodes

- The basic Consistent Hashing algorithm assigns a single token (or a consecutive hash range) to each physical node. 
- This was a static division of ranges that requires calculating tokens based on a given number of nodes.

**Here are a few potential issues associated with a manual and fixed division of the ranges**:

âœ“ **Adding or removing nodes**: Adding or removing nodes will result in recomputing the tokens causing a significant administrative overhead for a large cluster.

âœ“ **Hotspots**: Since each node is assigned one large range, if the data is not evenly distributed, some nodes can become hotspots.

âœ“ **Node rebuilding**: Since each nodeâ€™s data might be replicated (for fault-tolerance) on a fixed number of other nodes, when we need to rebuild a node, only its replica nodes can provide the data. This puts a lot of pressure on the replica nodes and can lead to service degradation.

- Instead of assigning a single token to a node, the hash range is divided into multiple smaller ranges, and each physical node is assigned several of these smaller ranges. 
- Each of these subranges is considered a Vnode. With Vnodes, instead of a node being responsible for just one token, it is responsible for many tokens (or subranges).

![image](https://user-images.githubusercontent.com/33947539/146916976-bce0eada-c4a7-4b98-ac5e-e9f64435e670.png)

The figure below shows how physical nodes A, B, C, D, & E use Vnodes of the Consistent Hash ring. Each physical node is assigned a set of Vnodes and each Vnode is replicated once.

![image](https://user-images.githubusercontent.com/33947539/146917124-28421651-8eaa-4415-be39-7d58372ae444.png)


#### Consistent Hashing helps with efficiently partitioning and replicating data; therefore, any distributed system that needs to scale up or down or wants to achieve high availability through data replication can utilize Consistent Hashing. A few such examples could be:

âœ“ Any system working with a set of storage (or database) servers and needs to scale up or down based on the usage, e.g., the system could need more storage during Christmas because of high traffic.

âœ“ Any distributed system that needs dynamic adjustment of its cache usage by adding or removing cache servers based on the traffic load.

âœ“ Any system that wants to replicate its data shards to achieve high availability.

>Amazonâ€™s Dynamo and Apache Cassandra use Consistent Hashing to distribute and replicate data across nodes.

