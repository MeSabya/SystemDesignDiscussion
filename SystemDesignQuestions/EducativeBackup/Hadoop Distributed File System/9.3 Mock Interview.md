## Question 1
How does HDFS scale?

Hide Answer
HDFS scales by adding DataNodes in the cluster. HDFS allows DataNodes to be added to a running cluster and offers tools to manually rebalance the data blocks when cluster nodes are added, which can be done without shutting the file system down.

Since NameNode stores all metadata in memory (for faster operations), HDFS is limited by how much memory the NameNode has. This limitation is handled through HDFS Federation, which was introduced in the 2.x release. HDFS Federation allows a cluster to scale by adding NameNodes, each of which manages a portion of the filesystem namespace.

## Question 2
How does HDFS ensure fault-tolerance and reliability of data?

Hide Answer
Fault tolerance in HDFS is achieved through data replication and maintaining a metadata transactions log for the NameNode.

Replication: HDFS replicates data to multiple nodes for reliability and fault tolerance. By default, each block has three replicas.

EditLog: For fault tolerance and in the event of NameNode crash, all metadata changes are written to the disk onto an edit log. This EditLog can also be replicated on a remote filesystem (e.g., NFS) or a secondary NameNode.

FsImage: The NameNode state is periodically serialized to disk and then replicated, so that on recovery, a NameNode may load the checkpoint into memory, replay any subsequent operations from the edit log, and be available again very quickly.

## Question 3
How does HDFS manage high availability?

Hide Answer
Each block is replicated onto multiple DataNodes to ensure high availability. By default, HDFS makes three replicas of each block.

To ensure maximum data availability, NameNode distributed replicas on different racks, so that clients can still read or write in case of a rack failure.

HDFS High Availability: To achieve high availability in case of a NameNode failure, Hadoop, in its 2.0 release, added support for HDFS High Availability (HA). In this implementation, there are two (or more) NameNodes in an active-standby configuration with a hot standby(s). At any point in time, exactly one of the NameNodes is in an active state, and the others are in a standby state. The active NameNode is responsible for all client operations in the cluster, while the standby is simply acting as a follower of the active, maintaining enough state to provide a fast failover when required.

## Question 4
How does HDFS perform NameNode failover?

Hide Answer
Older releases of HDFS require manual intervention for NameNode failover. After Hadoop 2.0, ZooKeeper is used to automatically failover in case of a NameNode crash. The ZKFailoverController (ZKFC) is a ZooKeeper client that runs on each NameNode (active and standbys) and is responsible for coordinating with the Zookeeper and also monitoring and managing the state of the NameNode.

## Question 5
How does HDFS ensure file data integrity?

Hide Answer
Data Integrity refers to ensuring the correctness of the data. The data received by a client from a DataNode could be corrupted. This corruption might happen due to problems in a storage device, network faults, or the software itself. HDFS uses checksum to verify the contents of files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace.

HDFS client, when retrieving file contents, first verifies that the data received from each DataNode matches the checksum stored in the associated checksum file. And if not, the client can opt to retrieve that block from another replica.

## Question 6
What is the use of Erasure Coding (EC) in HDFS?

Hide Answer
HDFS, by default, stores three copies of each block, resulting in a 200% overhead (to store two extra copies) in storage space and other resources (e.g., network bandwidth). Compared to this replication scheme, EC provides the same level of fault-tolerance with much less storage space. In a typical EC setup, the storage overhead is no more than 50%. This fundamentally doubles the storage space capacity by bringing down the replication factor from 3x to 1.5x.

Under EC, data is broken down into fragments, expanded, encoded with redundant data pieces, and stored across different DataNodes. If, at some point, data is lost on a DataNode due to corruption, etc., then it can be reconstructed using the other fragments stored on other DataNodes.

Although EC is more CPU intensive, it greatly reduces the storage needed for reliably storing a large data set.

## Question 7
Can HDFS handle small files efficiently?

Hide Answer
Because of its high-throughput design, HDFS lacks the ability to support the efficient random reading of small files. HDFS is designed to be used with large block sizes (128MB and larger). It is meant to take large files (hundreds of megabytes, gigabytes, or terabytes) and divide them into blocks, which can then be fed into MapReduce jobs for parallel processing. HDFS is inefficient when the actual file sizes are small (in the kilobyte range). Having a large number of small files places additional stress on the NameNode, which has to maintain metadata for all the files in the file system. Additionally, when there are a large number of files, there will be a lot of seeks on the disk as frequent hopping from data node to data node will be done, increasing the file read/write time.

Typically, HDFS users combine many small files into larger ones using techniques such as sequence files. A sequence file can be understood as a container of binary key-value pairs, where the file name is the key, and the file contents are the value.

## Question 8
How does HDFS handle DataNode failures?

Hide Answer
NameNode receives Heartbeat and BlockReport from each DataNode. Heartbeat receipt implies that the DataNode is alive and functioning correctly, and the BlockReport contains a list of all blocks on a DataNode. When NameNode observes that DataNode has not sent a heartbeat message after a certain amount of time, the DataNode is marked as dead. The NameNode replicates the blocks of the dead node to another DataNode. Hence, NameNode can easily handle DataNode failure.

## Question 9
How does HDFS separate control flow from data flow?

Hide Answer
HDFS is designed in such a way that clients never read or write data through the NameNode. Instead, a client asks the NameNode which DataNodes it should contact for reading a block. The client then contacts those DataNodes to access the data directly. Besides that, all communication between NameNode and DataNode, e.g., DataNode registration, BlockReport, Heartbeat, etc., is initiated by the DataNode, and responded to by the NameNode.
