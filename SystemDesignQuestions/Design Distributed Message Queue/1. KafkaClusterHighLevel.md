# What is Kafka?

Apache Kafka is an open-source publish-subscribe-based messaging system (Kafka can work as a message queue too, more on this later). 
It is distributed, durable, fault-tolerant, and highly scalable by design. Fundamentally, 
it is a system that takes streams of messages from applications known as producers, 
stores them reliably on a central cluster (containing a set of brokers), 
and allows those messages to be received by applications (known as consumers) that process the messages.

ðŸ‘‰ At a high level, we can call Kafka a **distributed Commit Log**. 
   A Commit Log **(also known as a Write-Ahead log or a Transactions log)** is an append-only data structure that can persistently store a sequence of records. 
   Records are always appended to the end of the log, and once added, records cannot be deleted or modified. Reading from a commit log always happens from left to right (or old to new).

ðŸ‘‰ Kafka stores all of its messages on disk. Since all reads and writes happen in sequence, Kafka takes advantage of sequential disk reads

## Why Kafka?

- One of the most powerful event streaming platforms available open source Offers solid horizontal scalability
- A perfect fit for big data projects involving real-time processing
- Durably stores the data using Distributed commit log meaning that the data is persisted on the disk
- High reliability, since it is distributed and replicated.
- Has excellent parallelism since the topics are partition.
- Unlike a traditional "dumb" message queue, Kafka lets consumers keep track of which messages have been read.


## High level Kafka Design

ðŸ‘‰ **These are four main parts in a Kafka system:**

- Broker: Handles all requests from clients (produce, consume, and metadata) and keeps data replicated within the cluster. There can be one or more brokers in a cluster.
- Zookeeper: Keeps the state of the cluster (brokers, topics, users).
- Producer: Sends records to a broker.
- Consumer: Consumes batches of records from the broker.

### Kafka Broker
A Kafka cluster consists of one or more servers (Kafka brokers) running Kafka. 
Producers are processes that push records into Kafka topics within the broker. A consumer pulls records off a Kafka topic.

#### Records
A record is a message or an event that gets stored in Kafka. Essentially, it is the data that travels from producer to consumer through Kafka. A record contains a key, a value, a timestamp, and optional metadata headers

#### Topics and Partitions

When an event stream enters Kafka, it is persisted as a topic. In Kafkaâ€™s universe, a topic is a materialized event stream. In other words, a topic is a stream at rest.

In simple terms, a topic is like a table in a database, and the messages are the rows in that table.

- Each message that Kafka receives from a producer is associated with a topic.
- Consumers can subscribe to a topic to get notified when new messages are added to that topic.
- A topic can have multiple subscribers that read messages from it.
- In a Kafka cluster, a topic is identified by its name and must be unique.
- Messages in a topic can be read as often as needed â€” unlike traditional messaging systems, messages are not deleted after consumption. Instead, Kafka retains messages for a configurable amount of time or until a storage size is exceeded. Kafkaâ€™s performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.

##### Kafka Topic Partitions and Segments

Kafkaâ€™s topics are divided into several partitions. While the topic is a logical concept in Kafka, a partition is the smallest storage unit that holds a subset of records owned by a topic. Each partition is a single log file where records are written to it in an append-only fashion.

Kafka brokers splits each partition into segments. Each segment is stored in a single data file on the disk attached to the broker. By default, each segment contains either 1 GB of data or a week of data, whichever limit is attained first. When the Kafka broker receives data for a partition, as the segment limit is reached, it will close the file and start a new one:

In the following diagram, the topic partition has 3 replicas, broker 2 is the leader replica, and brokers 1 and 3 are the follower replicas. Each of the replicas has a segment for the topic partition, which is backed up by its own file.

![image](https://user-images.githubusercontent.com/33947539/170002254-459d3777-f2bd-4f3c-81c2-7127f1039cc9.png)

When the data is written to a log segment, by default it is not flushed to disk immediately. Kafka relies on the underlying operating system to lazily flush the data to disk, which improves performance. Although this might appear to increase the risk of data loss, in most cases, each topic partition will be configured to have more than 1 replica. So, the data will exist on more than one broker.

## Producers
Producers are applications that publish (or write) records to Kafka.

## Consumers
Consumers are the applications that subscribe to (read and process) data from Kafka topics. Consumers subscribe to one or more topics and consume published messages by pulling data from the brokers.

In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. For example, producers never need to wait for consumers.

## ZooKeeper 
ðŸ‘‰ZooKeeper is a distributed **key-value store and is used for coordination and storing configurations**. 

ðŸ‘‰ It is highly optimized for reads. Kafka uses ZooKeeper to coordinate between Kafka brokers; 

ðŸ‘‰ ZooKeeper maintains metadata information about the Kafka cluster. We will be looking into this in detail later.

![image](https://user-images.githubusercontent.com/33947539/170005244-d5fa8718-02b2-4ac5-b3f9-41ef4ae92505.png)

![image](https://user-images.githubusercontent.com/33947539/170009993-d8bae021-4f5f-417a-92a4-2e028243ce3b.png)
![image](https://user-images.githubusercontent.com/33947539/170010018-2044eecc-c2f8-4d8b-8456-d3ff95efa9a9.png)


