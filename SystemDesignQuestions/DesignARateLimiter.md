# Design a Rate Limiter or a Circuit Breaker 

Important references: 
- https://systemsdesign.cloud/SystemDesign/RateLimiter
- https://medium.com/geekculture/system-design-design-a-rate-limiter-81d200c9d392

**Problem**: 
##### Design rate limiter â€” A rate limiter is a tool that monitors the number of requests per a window time a service agrees to allow. 
âœ“ **Functional Requirements**:

Limit the number of requests an entity can send to an API within a time window, e.g., 15 requests per second.
The APIs are accessible through a cluster, so the rate limit should be considered across different servers. The user should get an error message whenever the defined threshold is crossed within a single server or across a combination of servers.

âœ“ **Non-Functional Requirements**:

The system should be highly available. The rate limiter should always work since it protects our service from external attacks.
Our rate limiter should not introduce substantial latencies affecting the user experience.

![image](https://user-images.githubusercontent.com/33947539/149508833-688e97f1-99c4-4325-a0f6-ef3f1a4f15c4.png)


### Why do we need API rate limiting?

Rate Limiting helps to protect services against abusive behaviors targeting the application layer like Denial-of-service (DOS) attacks, brute-force password attempts, brute-force credit card transactions, etc. These attacks are usually a barrage of HTTP/S requests which may look like they are coming from real users, but are typically generated by machines (or bots). As a result, these attacks are often harder to detect and can more easily bring down a service, application, or an API.

**Rate limiters can be used,**

- For the client applications of your trading system: allow each client app to only send 5 orders per second
- Throttle number of rewards on payment platform per user per month, or/and a global limit on rewards per user
- For platforms like Medium: allow non-premium users to access only 3 premium stories per month
- Early Fraud detection if possible can be dropped from middleware before reaching the web servers

### Rate Limiting Strategies
**User**: 
A limit is applied on the number of requests allowed for a user in a given period of time. User based rate limiting is one of the most common & intuitive forms of rate limiting.

**Concurrency**: 
Here the limit is employed on the number of parallel sessions that can be allowed for a user in a given timeframe. A limit on the number of parallel connections helps mitigate DDOS attacks as well.

**Location/ID**: 
This helps in running location based or demography centric campaigns. Requests not from the target demography can be rate limited so as to increase availability in the target regions

**Server**:
Server based rate limiting is a niche strategy. This is employed generally when specific servers need most of the requests, i.e. servers are strongly coupled to specific functions

### How to do Rate Limiting?

**Rate Limiting** is a process that is used to define the rate and speed at which consumers can access APIs. 
**Throttling** is the process of controlling the usage of the APIs by customers during a given period. Throttling can be defined at the application level and/or API level. When a throttle limit is crossed, the server returns HTTP status â€œ429 - Too many requests".

### Different designs to implement rate limiters
- Token bucket algorithm
- Leaky bucket algorithm
- Fixed window counters
- Sliding Log algorithm
- Sliding window counters


#### Sliding Window Counter Algorithm
For example, in order to build a rate limiter of 100 req/hr, say a bucket size of 20 mins is chosen, then there are 3 buckets in the unit time
For a window time of 2AM to 3AM, the buckets are
```python
{
 "2AM-2:20AM": 10,
 "2:20AM-2:40AM": 20,
 "2:40AM-3:00AM": 30
}
```
If a request is received at 2:50AM, we find out the total requests in last 3 buckets including the current and add them, in this case they sum upto 60 (<100), so a new request is added to the bucket of 2:40AMâ€“3:00AM givingâ€¦
```python
{
 "2AM-2:20AM": 10,
 "2:20AM-2:40AM": 20,
 "2:40AM-3:00AM": 31
}
```

##### Python implementation of above algorithm is :

```python
import time
import threading

class RequestCounters(object):
	# Every window time is broken down to 60 parts
	# 100 req/min translates to requests = 100 and windowTimeInSec = 60
	def __init__(self, requests, windowTimeInSec, bucketSize=10):
		self.counts = {}             # A map which maintains bucket Number as a key and number of requests as value 
		self.totalCounts = 0         # Total number of requests 
		self.requests = requests
		self.windowTimeInSec = windowTimeInSec
		self.bucketSize = bucketSize
		self.lock = threading.Lock()

	# Gets the bucket for the timestamp
	def getBucket(self, timestamp):
		factor = self.windowTimeInSec / self.bucketSize
		return (timestamp // factor) * factor

	# Gets the bucket list corresponding to the current time window
	def _getOldestvalidBucket(self, currentTimestamp):
		return self.getBucket(currentTimestamp - self.windowTimeInSec)

	# Remove all the older buckets that are not relevant anymore
	def evictOlderBuckets(self, currentTimestamp):
		oldestValidBucket = self._getOldestvalidBucket(currentTimestamp)
		bucketsToBeDeleted = filter(
			lambda bucket: bucket < oldestValidBucket, self.counts.keys())
		for bucket in bucketsToBeDeleted:
			bucketCount = self.counts[bucket]
			self.totalCounts -= bucketCount
			del self.counts[bucket]

class SlidingWindowCounterRateLimiter(object):
	def __init__(self):
		self.lock = threading.Lock()
		self.ratelimiterMap = {}

	# Default of 100 req/minute
	# Add a new user with a request rate
	# If a request from un-registered user comes, we throw an Exception
	def addUser(self, userId, requests=100, windowTimeInSec=60):
		with self.lock:
			if userId in self.ratelimiterMap:
				raise Exception("User already present")
			self.ratelimiterMap[userId] = RequestCounters(requests, windowTimeInSec)

	def removeUser(self, userId):
		with self.lock:
			if userId in self.ratelimiterMap:
				del self.ratelimiterMap[userId]

	@classmethod
	def getCurrentTimestampInSec(cls):
		return int(round(time.time()))

	def shouldAllowServiceCall(self, userId):
		with self.lock:
			if userId not in self.ratelimiterMap:
				raise Exception("User is not present")
		userTimestamps = self.ratelimiterMap[userId]
		with userTimestamps.lock:
			currentTimestamp = self.getCurrentTimestampInSec()
			# remove all the existing older timestamps
			userTimestamps.evictOlderBuckets(currentTimestamp)
			currentBucket = userTimestamps.getBucket(currentTimestamp)
			userTimestamps.counts[currentBucket] = userTimestamps.counts.
				get(currentBucket, 0) + 1
			userTimestamps.totalCounts += 1
			if userTimestamps.totalCounts > userTimestamps.requests:
				return False
			return True
```
### Rate Limiting in Distributed Systems
The above algorithms works very well for single server applications. But the problem becomes very complicated when there is a distributed system involved with multiple nodes or app servers.It becomes more complicated if there are multiple rate limited services distributed across different server regions. The two broad problems that comes across in these situations are Inconsistency and Race Conditions.

### Scenario: Distributed API Gateway Rate Limiting
You run a global API service â€” api.example.com â€” deployed across 3 regions:

- us-east-1
- eu-west-1
- ap-southeast-1

Each region has multiple load-balanced app servers, and you want to enforce this rate limit per user:

Each user is allowed 100 requests per minute globally, no matter which region or server they hit.

### Problem: Inconsistency in Distributed Rate Limiters
Letâ€™s say a user sends:

- 40 requests to us-east-1
- 40 requests to eu-west-1
- 40 requests to ap-southeast-1

Each region is enforcing its own 100 RPM limit, so none of them reject the user.

But globally, the user sent 120 requests, and no server rejected them. This violates your limit.

This happens because each region is unaware of other regionsâ€™ counters. Thatâ€™s inconsistency.

### Problem: Race Conditions in Shared Store

Letâ€™s say instead you use Redis as a centralized counter store and all app servers increment this shared counter.

But hereâ€™s what can happen:

- 10 parallel requests come in at the same time.
- All of them read the current counter value: count = 98.
- All decide itâ€™s under the limit, so they increment it.
- Final value becomes 108, violating the limit.
- This happens because of a race condition: multiple reads and writes aren't synchronized.

### âœ… Solution 1: Sticky Sessions (Load Balancer Affinity)
What happens:

- You configure the load balancer to stick a user to a specific server (e.g., based on a cookie).
- All requests from a user during a session go to only one app server.
- That server maintains local rate limiter (e.g., token bucket or leaky bucket).
-  Pros:
-  Simple and fast (no cross-node calls).
-  No need for centralized data store.

- Cons:
- Scalability issue: If a user sends too many requests, that server gets overloaded.
- Fault tolerance issue: If the server dies, rate limit state is lost.
- Doesn't work well with auto-scaling or stateless systems.

### âœ… Solution 2: Centralized Store (Redis)
You move rate limit counters to a centralized, strongly consistent store like Redis.

For example:

- Key: rate_limit:{user_id}:{window_start}
- Value: request count
- TTL: set to end of window (e.g., 60 seconds)

Each request does:

```text
INCR rate_limit:{user_id}:{window_start}
```

And checks if value â‰¤ 100.

ðŸ”§ Pros:
Globally consistent

Easily shareable across regions and nodes

ðŸš« Cons:
Introduces latency

Requires careful locking or atomic ops to avoid race conditions

#### âœ… Race Condition Mitigation: Atomic Operations
To avoid race conditions in Redis:

Use atomic commands like INCR, INCRBY, or Lua scripts.

Or use Redis' SETNX (Set if Not Exists) for first-time entries.

Optionally wrap in Redis transactions (MULTI/EXEC) or Lua script:

lua
Copy
Edit
local current = redis.call("INCR", KEYS[1])
if current == 1 then
  redis.call("EXPIRE", KEYS[1], ARGV[1])
end
return current

### Data Sharding and Caching

- We can shard based on the â€˜UserIDâ€™ to distribute the userâ€™s data. For fault tolerance and replication we should use Consistent Hashing. 
- If we want to have different throttling limits for different APIs, we can choose to shard per user per API. 
- Take the example of URL Shortener; we can have different rate limiter for createURL() and deleteURL() APIs for each user or IP.

- Our system can get huge benefits from caching recent active users. Application servers can quickly check if the cache has the desired record before hitting backend servers. 
- Our rate limiter can significantly benefit from the Write-back cache by updating all counters and timestamps in cache only. 
- The write to the permanent storage can be done at fixed intervals.
- The reads can always hit the cache first; which will be extremely useful once the user has hit their maximum limit and the rate limiter will only be reading data without any updates.
- Least Recently Used (LRU) can be a reasonable cache eviction policy for our system.


### Should we rate limit by IP or by user?

***IP***: In this scheme, we throttle requests per-IP. 
The biggest problem with IP based throttling is when multiple users share a single public IP like in an internet cafe or smartphone users that are using the same gateway. One bad user can cause throttling to other users. Another issue could arise while caching IP-based limits, as there are a huge number of IPv6 addresses available to a hacker from even one computer, itâ€™s trivial to make a server run out of memory tracking IPv6 addresses!

***User***:
The biggest problem with IP based throttling is when multiple users share a single public IP like in an internet cafe or smartphone users that are using the same gateway. One bad user can cause throttling to other users. Another issue could arise while caching IP-based limits, as there are a huge number of IPv6 addresses available to a hacker from even one computer, itâ€™s trivial to make a server run out of memory tracking IPv6 addresses!

***Hybrid***: 
A right approach could be to do both per-IP and per-user rate limiting, as they both have weaknesses when implemented alone, 


![image](https://github.com/user-attachments/assets/2d2603e7-a592-400a-9199-097abf8ab437)



