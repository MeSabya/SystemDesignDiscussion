# Logging in a distributed system
In todayâ€™s world, an increasing number of designs are moving to microservice architecture instead of monolithic architecture. In microservice architecture, logs of each microservice are accumulated in the respective machine. If we want to know about a certain event that was processed by several microservices, it is difficult to go into every node, figure out the flow, and view error messages. But, it becomes handy if we can trace the log for any particular flow from end to end.

Moreover, it is also not necessary that a microservice is deployed on only one node. It can be deployed on thousands of nodes. 

In real world production environment, there would be running so many of machines containing hundreds of containers that can be terminated, restarted, or rescheduled at any point in time. This is the nature containerisation of the system is a challenge in itself and better think about how to obtain application logs that not to be lost.

There are three problems that need to be solved here. 

- How to ship logs from the machine
- How to process and save it 
- Show or visualize it.

![image](https://user-images.githubusercontent.com/33947539/193112988-a21696e8-7a5f-437d-b4a0-e12af563ea69.png)


***Kubernetes architecture provide a number of ways to manage application logs. Some of them approaches to consider are:***

ðŸ‘‰ **Native mode**: Run the kubectl logs command to directly view the local logs or use the log driver of Docker Engine to redirect logs to files, syslog, or Fluentd.

ðŸ‘‰ **DaemonSet mode**: A log agent is deployed on every Kubernetes node. The log agent collects the container logs and sends them to the server.

ðŸ‘‰ **SideCar mode**: A Pod runs a SideCar log agent container to collect logs generated by the primary container in the Pod.

![image](https://user-images.githubusercontent.com/33947539/183357295-f082752a-a898-4f92-aead-cd2a3ac5bea4.png)

## Using a Node-Level Logging Agent or Daemonset Mode
In this approach, you deploy a node-level logging agent on each node of your cluster. This agent is usually a container with access to log files of all application containers running on that node. Production clusters normally have more than one nodes spun up. If this is your case, youâ€™ll need to deploy a logging agent on each node.

The easiest way to do this in Kubernetes is to create a special type of deployment called **DaemonSet**. 
The DaemonSet controller will ensure that for every node running in your cluster you have a copy of the logging agent pod. The DaemonSet controller will also periodically check the count of nodes in the cluster and spin up/down a logging agent when the node count changes. 

DaemonSet structure is particularly suitable for logging solutions because you create only one logging agent per node and do not need to change the applications running on the node. 

ðŸ‘‰ *The limitation of this approach, however, is that node-level logging only works for applicationsâ€™ standard output and standard error streams.*

![image](https://user-images.githubusercontent.com/33947539/183387240-5612cad9-50bd-4d72-9ead-646f1b4d06cd.png)

![image](https://user-images.githubusercontent.com/33947539/183393678-6f2879a2-2eb9-4f25-b3bd-85ee9d266b4b.png)



## Detailed Components Design 
### Docker: 
Is our core containerization technology, though the fundamentals of this article will apply equally well if youâ€™re using an alternative. With Docker, everything published by a containerized application to stdout or stderr can be easily retrieved and re-routed by our logging agent.

### Fluentd:
will serve as our logging driver, it will parse and format the messages from each Docker container before publishing them to the queue. This will take the place of Logstash in the ELK stack because itâ€™s easier to set up with Docker and generally (as of 2020) a more performant option.
Kafka will handle our queues, itâ€™s designed for logging performance with a throughput of up to 2 million writes per second on a single shard. Kafka is a very powerful piece of software, allowing for configurable shards, delivery strategies, and fault-tolerance automatic recovery, and forms the backbone of most scalable log aggregation systems.

#### Overview:

- data collector tool
- The unifying later between different types of log inputs and outputs (Unified Logging Layer)
- unified logging with JSON
- Fluentd tries to structure data as JSON as much as possible
- collecting, filtering, buffering, outputting logs across multiple sources

ðŸ‘‰ **some relevant data sources:**

Applications logs: Java, .Net, Python, Perl, Ruby, Node.js, PHP

Network protocols: HTTP, TCP, UDP, Unix Domain Socket

Others: Docker, Kubernetes, Kafka, Syslog, Windows Event Log

ðŸ‘‰ **some relevant data outputs:**

Log Management: Splunk, ElasticSearch

Big Data: MongoDB, Hadoop FS

PubSub/Queue: Kafka

Data Warehouse: SQL Server, MySQL, PostgreSQL

Middleware: Couchbase

![image](https://user-images.githubusercontent.com/33947539/183388243-f56a37c5-1997-4745-8c60-5647736af6ea.png)\

ðŸ‘‰ ***Why FluentD ?***

For Kubernetes environments, Fluentd seems the ideal candidate due to its built-in Docker logging driver and parser â€“ which doesnâ€™t require an extra agent to be present on the container to push logs to Fluentd. In comparison with Logstash, this makes the architecture less complex and also makes it less risky for logging mistakes. The fact that Fluentd, like Kubernetes, is another CNCF project is also an added bonus!


### Zookeeper 
To run Kafka and handle distributed synchronization as well as providing group services, naming, etc for our Kafka instances.

### Kafka Connect 
is a Kafka component that allows Kafka to communicate with external services like key-value stores and databases. In our case, weâ€™ll be using it to connect Kafka to ElasticSearch and publish our logs.

### ElasticSearch 
will serve as our primary datastore and search engine for our logs. Itâ€™s pretty much the industry standard for indexing and searching unstructured data, which has made it a mainstay of logging solutions for the past decade.

### Kibana 
will manage the visualization and exploration of our data. It also has a powerful actions plugin, Kibana Alerting, which is a great option for managing alerting policy.

![image](https://user-images.githubusercontent.com/33947539/183392605-be95a451-702c-434b-adcd-c8814c1c6ddd.png)

## Summary of FluentD setup:

### FluentD Setup per node:
========================

Step1. Grant Permissions to Fluentd
------------------------------------
 
Fluentd will be collecting logs both from user applications and cluster components such as kube-apiserver and kube-scheduler, so we need to grant it some permissions.

The first thing we need to do is to create an identity for the future Fluentd DaemonSet . Letâ€™s create a new ServiceAccount in the kube-system namespace where Fluentd should be deployed:


Step2. letâ€™s grant Fluentd permissions to read, list, and watch pods and namespaces in your Kubernetes cluster
----------------------------------------------------------------------------------------------------------------

By creating a cluster role.

Step3: Bind the cluster role and service accountusing cluster role binding
--------------------------------------------------------------------------

Step4: Launch FluentD DaemonSet , add serviceAccount and serviceAccountName there as created above. Used the plugin fluent-plugin-elasticsearch to write and index the logs in Elastic.

Step5: In case you need to modify FluentD configs , create another config file and attach it to the created Daemonset.

Step6: Need to have a Kibana setup to see the FluenD logs.



## Reference:
https://medium.com/kubernetes-tutorials/cluster-level-logging-in-kubernetes-with-fluentd-e59aa2b6093a
https://leetcode.com/discuss/interview-question/system-design/622704/Design-a-system-to-store-and-retrieve-logs-for-all-of-eBay
https://dzone.com/articles/distributed-logging-architecture-for-microservices

