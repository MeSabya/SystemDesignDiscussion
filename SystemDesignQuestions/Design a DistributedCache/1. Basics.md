*A cache is added to the system to deal with performance deterioration. 
A cache is a temporary data storage that can serve data faster by keeping data entries in memory. Caches store only the most frequently accessed data. 
When a request reaches the serving host, it retrieves data from the cache (cache hit) and serves the user. 
However, if the data is unavailable in the cache (cache miss), the data will be queried from the database. 
Also, the cache is populated with the new value to avoid cache misses for the next time.*

![image](https://user-images.githubusercontent.com/33947539/203305692-4a091674-251e-44c4-a8bc-940de3e32c21.png)

#### Generally, distributed caches are beneficial in the following ways:

- They minimize user-perceived latency by precalculating results and storing frequently accessed data.
- They pre-generate expensive queries from the database.
- They store user session data temporarily.
- They serve data from temporary storage even if the data store is down temporarily.
- Finally, they reduce network costs by serving data from local resources.

## Writing policies
Often, cache stores a copy (or part) of data, which is persistently stored in a data store. When we store data to the data store, some important questions arise:

- Where do we store the data first? Database or cache?
- What will be the implication of each strategy for consistency models?

The short answer is, it depends on the application requirements. Letâ€™s look at the details of different writing policies to understand the concept better:

- **Write-through cache**: The write-through mechanism writes on the cache as well as on the database. Writing on both storages can happen concurrently or one after the other. This increases the write latency but ensures strong consistency between the database and the cache.

- **Write-back cache**: In the write-back cache mechanism, the data is first written to the cache and asynchronously written to the database. Although the cache has updated data, inconsistency is inevitable in scenarios where a client reads stale data from the database. However, systems using this strategy will have small writing latency.

- **Write-around cache**: This strategy involves writing data to the database only. Later, when a read is triggered for the data, itâ€™s written to cache after a cache miss. The database will have updated data, but such a strategy isnâ€™t favorable for reading recently updated data.

## Eviction policies
One of the main reasons caches perform fast is that theyâ€™re small. Small caches mean limited storage capacity. Therefore, we need an eviction mechanism to remove less frequently accessed data from the cache.

Several well-known strategies are used to evict data from the cache. The most well-known strategies include the following:

- Least recently used (LRU)
- Most recently used (MRU)
- Least frequently used (LFU)
- Most frequently used (MFU)

Other strategies like first in, first out (FIFO) also exist. The choice of each of these algorithms depends on the system the cache is being developed for


## Storage mechanism

Storing data in the cache isnâ€™t as trivial as it seems because the distributed cache has multiple cache servers. When we use multiple cache servers, the following design questions need to be answered:

ðŸ‘‰ Which data should we store in which cache servers?

ðŸ‘‰ What data structure should we use to store the data?

The above two questions are important design issues because theyâ€™ll decide the performance of our distributed cache, which is the most important requirement for us. Weâ€™ll use the following techniques to answer the questions above.

### Hash function
Itâ€™s possible to use hashing in two different scenarios:

ðŸ‘‰ Identify the cache server in a distributed cache to store and retrieve data.

ðŸ‘‰ Locate cache entries inside each cache server.

For the **first scenario**, we can use different hashing algorithms. However, consistent hashing or its flavors usually perform well in distributed systems because simple hashing wonâ€™t be ideal in case of crashes or scaling.

In the **second scenario**, we can use typical hash functions to locate a cache entry to read or write inside a cache server. However, a hash function alone can only locate a cache entry. It doesnâ€™t say anything about managing data within the cache server. That is, it doesnâ€™t say anything about how to implement a strategy to evict less frequently accessed data from the cache server. It also doesnâ€™t say anything about what data structures are used to store the data within the cache servers. This is exactly the second design question of the storage mechanism. Letâ€™s take a look at the data structure next.

### Linked list
Weâ€™ll use a doubly linked list. The main reason is its widespread usage and simplicity. Furthermore, adding and removing data from the doubly linked list in our case will be a constant time operation. This is because we either evict a specific entry from the tail of the linked list or relocate an entry to the head of the doubly linked list. Therefore, no iterations are required.

Note: Bloom filters are an interesting choice for quickly finding if a cache entry doesnâ€™t exist in the cache servers. We can use bloom filters to determine that a cache entry is definitely not present in the cache server, but the possibility of its presence is probabilistic. Bloom filters are quite useful in large caching or database systems.

## Cache client
We discussed that the hash functions should be used for the selection of cache servers. But what entity performs these hash calculations?

A cache client is a piece of code residing in hosting servers that do (hash) computations to store and retrieve data in the cache servers. Also, cache clients may coordinate with other system components like monitoring and configuration services. All cache clients are programmed in the same way so that the same PUT, and GET operations from different clients return the same results. Some of the characteristics of cache clients are the following:

Each cache client will know about all the cache servers.
All clients can use well-known transport protocols like TCP or UDP to talk to the cache servers.

![image](https://user-images.githubusercontent.com/33947539/203362197-d42330ad-27a4-41e1-be6c-7f83c37c85e0.png)

The main components in this high-level design are the following:

Cache client: This library resides in the service application servers. It holds all the information regarding cache servers. The cache client will choose one of the cache servers using a hash and search algorithm for each incoming insert and retrieve request. All the cache clients should have a consistent view of all the cache servers. Also, the resolution technique to move data to and from the cache servers should be the same. Otherwise, different clients will request different servers for the same data.

Cache servers: These servers maintain the cache of the data. Each cache server is accessible by all the cache clients. Each server is connected to the database to store or retrieve data. Cache clients use TCP or UDP protocol to perform data transfer to or from the cache servers. However, if any cache server is down, requests to those servers are resolved as a missed cache by the cache clients.




