# The Life of Dynamoâ€™s put() & get() Operations

## Strategies for choosing the coordinator node
Dynamo clients can use one of the two strategies to choose a node for their get() and put() requests:

ðŸ‘‰ Clients can route their requests through a generic load balancer.

ðŸ‘‰ Clients can use a partition-aware client library that routes the requests to the appropriate coordinator nodes with lower latency.

In the first strategy, the client is unaware of the Dynamo ring, which helps scalability and makes Dynamoâ€™s architecture loosely coupled. However, in this case, since the load balancer can forward the request to any node in the ring, it is possible that the node it selects is not part of the preference list. This will result in an extra hop, as the request will then be forwarded to one of the nodes in the preference list by the intermediate node.

The second strategy helps in achieving lower latency, as in this case, the client maintains a copy of the ring and forwards the request to an appropriate node from the preference list. Because of this option, Dynamo is also called a zero-hop DHT, as the client can directly contact the node that holds the required data. However, in this case, Dynamo does not have much control over the load distribution and request handling.

![image](https://user-images.githubusercontent.com/33947539/154466017-2f4a8626-ae77-4603-9f97-78839f1a5914.png)

## Consistency protocol

Dynamo uses a consistency protocol similar to quorum systems. If R/WR/W is the minimum number of nodes that must participate in a successful read/write operation respectively:

Then R+W > NR+W>N yields a quorum-like system
A Common (N, R, WN,R,W) configuration used by Dynamo is (3, 2, 2).

- (3, 3, 1): fast WW, slow RR, not very durable
- (3, 1, 3): fast RR, slow WW, durable

In this model, the latency of a get() (or put()) operation depends upon the slowest of the replicas. For this reason, RR and WW are usually configured to be less than NN to provide better latency.
In general, low values of WW and RR increase the risk of inconsistency, as write requests are deemed successful and returned to the clients even if a majority of replicas have not processed them. This also introduces a vulnerability window for durability when a write request is successfully returned to the client even though it has been persisted at only a small number of nodes.
For both Read and Write operations, the requests are forwarded to the first â€˜Nâ€™ healthy nodes. This is what is called as sloppy quoarum.

## â€˜put()â€™ process
Dynamoâ€™s put() request will go through the following steps:

The coordinator generates a new data version and vector clock component.
Saves new data locally.
Sends the write request to Nâˆ’1 highest-ranked healthy nodes from the preference list.
The put() operation is considered successful after receiving W-1 confirmation.

## â€˜get()â€™ process
Dynamoâ€™s get() request will go through the following steps:

The coordinator requests the data version from N-1 highest-ranked healthy nodes from the preference list.
Waits until R-1 replies.
Coordinator handles causal data versions through a vector clock.
Returns all relevant data versions to the caller.
