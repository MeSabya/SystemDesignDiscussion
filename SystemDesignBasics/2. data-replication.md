Copying the same data over multiple nodes in distributed systems is critical to keep the database up and keep on serving queries even during faults.
Following are other reason behind data replication in a distributed system :

## Why do we need it?

Let‚Äôs say If I am the CEO of some e-commerce startup based in the USA. Now I want to cater to orders globally like from people living in Antarctica :P But my servers are running in the USA. So when a user comes to my application and searches an item it gets a lot of time to display the final results on his browser. I don‚Äôt think increasing resources on my server is going to solve this problem. Isn‚Äôt it?
So what‚Äôs the root cause of the problem here?

üëâ Yes, You got it right. it‚Äôs a **network latency issue**. 

So by replication, we can keep data close to the user‚Äôs geographic area to reduce latency. 
Similar things are done by Netflix and Amazon prime video etc that‚Äôs why you people can watch HD movies without any buffering delays.

Similarly, other problems which replication caters are :
 
          üëâ **Availability & Fault toleration issues**

          üëâ **Data throughput issues**

Let‚Äôs again consider a real-time scenario. So as a CEO of an e-commerce startup, if my application goes down (Unavailable) then my whole business could get impacted and I could have lost thousands of bucks and can lose the trust of our customers which is very bad these days. Hence we need better **Fault tolerance** in our system.

Even if my startup gets very much popular like Amazon etc then my server would be handling millions of transactions/queries per second (TPS/QPS). And if my system is not that much scalable then user experience would be deteriorated. The quality of service (QOS) will be very poor. No one would like to visit our application.
Hence we need to make our system more Scalable and need to have better Data throughput.

## How does replication work?
There are three ways through which leaders replicate data to its follower :
1. Statement-based replication
2. Write-Ahead log
3. Row-based replication

### Statement-based replication
In this strategy leader logs every write statement(INSERT/UPDATE/DELETE) that it executes and sends the same to its follower and then the follower executes it on its node.
#### Drawback :
Any non-deterministic function can result in different writes on follower and leader(Like RAND() or NOW() functions). 

### Write-Ahead log
Whenever a query comes to a system, even before executing that query, 
it is written in an append-only log file also known as Write-ahead log file. This log file can be used to replicate data to follower nodes.
PostgreSQL and Oracle employ this strategy of data replication.

### Row-based replication
A row-based replication involves a sequence of records describing writes to the level of row. For example, an insertion of row contains information 
about all new values of columns. A delete contains information to identify rows to be deleted and an update contains new values of all columns.

**Approaches to data replication**
- Single leader
- Multi leader
- leaderless

## Single leader replication (Active-Passive or Master-Slave Replication)
- So in leader based architecture, client (application server) requests are sent to **leader DB first** and after that leader sends the data changes to all of its followers as a   part of the replication log.

- Whenever a client wants to read data from the database then it can query either leader or any of the follower (Yes there is generally more than just one follower to make the 
  system highly available). However, writes to the database is only accepted on the leader by the client.
  
- Now whenever a follower dies our application will not get impacted as there is not just a single node of data. Our application can read from other followers as well and hence   this makes our system highly Read Scalable
- Used in: PostgreSQL, MySql, SQL Server, MongoDB, Kafka, etc.

- ![image](https://user-images.githubusercontent.com/33947539/157592021-92d4abbf-9ef8-44dd-85e7-d7614ca12b00.png)

![image](https://user-images.githubusercontent.com/33947539/157592385-938313b7-36ef-4f38-99bc-d210b5ddb576.png)

### Points to Ponder over ü§î

ü§îOne of the crucial points you might be thinking is that what if there is a delay in replicating data from leader to follower and a client reads from a follower?

Yes, there is a high possibility for this scenario to occur but stale reads are acceptable and this is inevitable because as per the CAP Theorem  you can achieve only two guarantees out of Consistency, Partition Tolerance, Availability at a time. Which means stale reads are acceptable. This is known as Eventual-Consistency. Even read-heavy systems like Facebook or Instagram are eventually consistent.

***Replication strategies in Master-Slave Replication***
        1. Synchronous replication
        2. Asynchronous replication

### Synchronous replication strategy in Master-slave architecture
- In this strategy, followers are guaranteed to have an up to date copy of data (Strong Consistency) which is an advantage.

- But one of the biggest disadvantages it has is that it will block the client until the leader receives the OK response from all the followers. Now if you have a very high read scalable system like Facebook with thousands of followers nodes then waiting for data to be replicated at each node is not a good user experience.

### Asynchronous replication strategy in Master-slave architecture

- This strategy will not block the user/client for the response when they want to write data to the leader but yes it comes with the caveat of eventual consistency.
- One disadvantage I can think of this strategy right now is that if by some issue followers aren‚Äôt accepting changes from the leader (might be network problem) then any new    
  writes by the client will be lost if the leader itself goes down. Hence writes are not Durable sometimes which is a trade-off in this strategy.

## Handling Node outages in replication  
So there are two scenarios as mentioned below:
    
    Follower failure
    Leader failure
    
**Follower failure**

So in the scenario of follower failure, we can use a strategy called **Catchup recovery**. 
In this strategy, the follower (which got disconnected) can connect to the leader again and request all data changes that occur when the follower was disconnected.

**Leader failure**

Now in the scenario of Leader failure, we can use a strategy called **Failover**. In this strategy, One of the followers needs to be promoted as a new leader.

üëâ***Now you people might be wondering which follower to make the leader? How is it decided?***

So yes there is a voting algorithm which is called **Consensus Algorithm**. 
In layman terms, in this algorithm, we have Quorum of followers and all the followers decide which follower should be made a leader.  
  

## Multi-Leader Replication

### Issues in Leader based Replication

Let‚Äôs take a real-time scenario to understand the issue in Leader based Replication.
Again I will extend my previous blog example here. So as the CEO of an e-commerce startup based in the USA, I wanted to cater to orders from people living in Antarctica :P
No issues, as a solution what I can do is to place some of my database servers (Followers) in Antarctica itself to reduce the Read Latency of my application (As customers are equivalent to God :P) but there is a limitation here. Take some time and think about it.

Yes, you guessed it correct what about **Write Latency**.

So as we know in leader based replication all writes must go through the leader itself and there is only a single leader in it.
Now I have two options, whether to place my leader in the data center of Antarctica or to place my leader in the data center of the USA (near the headquarters).
So, in either case, customers coming to place orders (Write operation for DB) from the different geographic areas are going to suffer Write Latency. Which in turn can impact the performance of my system again.
Also what if the data center having the leader (containing latest changed) fails and those changes were not replicated to all the followers in other data centers?
So are you able to see the flaws in our current implementation? Yes, I am trying to create a basic requirement to have a better solution. 

![image](https://user-images.githubusercontent.com/33947539/157592335-ada065b2-c771-4ecb-907d-f3a88046d8a0.png)


**Indeed that‚Äôs Multi-Leader Replication.**

So in Multi-Leader replication, we are going to have one leader in each of my data centers and each data center‚Äôs leader replicates its changes to the leaders in other data centers **asynchronously.**

### Advantages of Multi-Leader Replication

**Better Performance** as compared to Single leader replication as we have now reduced both Read & Write Latency of our application

**High Fault Tolerance** as each data center can continue operating independently of others if any data center goes down. This is possible because each data center has its leader. Also, replication catches up when the failed datacenter comes back online

### Multi-Leader Replication used in our daily lives

E:g Google Calendar, Outlook Calendar, Google sheets, Google docs, Microsoft Excel etc.

So when you make any change while you are offline in the calendar app like you booking a slot for the meeting.
Now those changes need to be synced (replicated) with the server (including other devices on which app is installed) when our device gets online back. So every device database can act as a dynamic leader and performs Async Multi-leader replication..

### When to Use Multi-Leader Replication Strategy?

üëâ Multi-datacenter operation

As discussed above in the section : [issues-in-Single leader-based-replication](https://github.com/MeSabya/SystemDesignDiscussion/edit/main/SystemDesignBasics/2.%20data-replication.md#issues-in-leader-based-replication)

üëâ Offline-mode Support
Another use case is to support offline mode. The offline mode is prevalent nowadays in a lot of services (e.g., Google Maps, Evernote). It allows users to continue using services without a network connection.

To make this happen, your local machine/device would be acting as another write replica and become responsible for handling write requests and syncing the data to other machines when the network connection comes back online. Such setup could look like this:

![image](https://user-images.githubusercontent.com/33947539/157593018-91ecfc05-28a6-4b9f-a5a2-ecf5108ca7b9.png)


üëâ Collaborative Editing

When you are using a collaborative editing tool, you would have noticed that your edits are instantly applied. This is possible because once again, your machine is acting as a write replica to handle edit requests and send updates asynchronously to other machines.

You might have seen an error message like ‚ÄúSomething went wrong, please refresh.‚Äù Such error can happen when multiple users attempt to make updates to the same data and the system cannot automatically resolve the write conflicts.

![image](https://user-images.githubusercontent.com/33947539/157593252-004c8f0a-ad54-410b-92b2-bee2497c5b39.png)

### What‚Äôs the Complexity in Using Multi-Leader Setup?

Two main complexities in using the multi-leader algorithm:

üëâ Handling write-conflicts

üëâ Inconsistent read

#### Handling Write Conflicts

*What is a conflict? Imagine a situation where two users are trying to change the title of a document simultaneously and the edit requests are handled by two different leader nodes. How do you know in what order to apply the edit request or how to merge two requests?*

There are multiple approaches to handling write conflicts. For instance, you can configure to avoid write conflicts by making sure the same data is handled by the same leader node. Or you can decide to implement some algorithm to automatically resolve conflicts in a consistent manner (e.g., using a unique ID composed of timestamp and randomly generated number), which would potentially incur data loss.

#### Inconsistent Read

Another problem with the multi-leader setup is handling inconsistent read. The replication updates are done asynchronously so we can potentially end up with a situation where two nodes have different views of the data. If a user somehow ends up sending read requests to both machines, they might end up with reading inconsistent data:

![image](https://user-images.githubusercontent.com/33947539/157603043-dc7cf547-c0d9-4d11-bfe4-2e5a52bc8d12.png)


# Leaderless Replication
In a leader-centric replication, there is a Master node that accepts the writes. Upon applying the writes on its copy of data, the database engine sends out the updates across read-replicas or master nodes. Given that all the writes flow through the Master node, the order of the writes is deterministic, slimming down the chances of having a write conflict.

Leader-centric replication is not fault-tolerant by design because we lose the write operation when the Master node is down. Leaderless replication addresses this concern and ensures our system can handle Write operations even when a subset of nodes are having an outage.

Leaderless Replication eradicates the need of having a leader accepting the writes; instead, it leverages quorum to ensure strong consistency across multiple nodes and good tolerance to failures.

In some leaderless implementations

the client directly sends its writes to several replicas.

while in others, a coordinator node  sends the writes to the replicas on behalf of the clients. Unlike a leader-based replication, the coordinator does not enforce a particular ordering of writes. And this difference in design has profound consequences for the way the leaderless architecture is used.

***Leaderless Replication does NOT enforce particular ordering of writes.***

***In a leaderless configuration failover does not exist.***

## Quorum Consensus
Quorum Consensus says, if there are n replicas, every write must be confirmed by w nodes to be considered as successful, and we must query at least r nodes for each read. As long as w + r > n, we expect to get an up-to-date value when reading, because at least one of the r  nodes we are reading from must be up-to-date. Reads and writes that obey r and w values are called quorum reads and writes. We can think of r  and w as the number of votes required for the read and write to be valid.

***What matters in Quorum Consistency is that the nodes used in reads and writes have at least one node in common (i.e., overlapping).***

### Approaches for Leaderless Replication:
#### Client-driven fan-out
client was sending reads and writes to all the replicated data nodes and, depending on the quorum configuration, decides the correctness. This approach is called client-driven fan-out and is very popular.

![image](https://user-images.githubusercontent.com/33947539/175820984-7e4a55fe-dc6f-434e-87a2-f1d33d6622cb.png)

#### Node Coordinator
The client will make the request to any one node, and it then starts to act as the coordinator for that transaction. This node coordinator will then take care of the fan-out to other nodes and complete the transaction. Upon completion, it returns the response to the client.

![image](https://user-images.githubusercontent.com/33947539/175821007-c2ee3732-92ae-4271-a1fa-f886567ba213.png)

Apache Cassandra uses this approach for implementing Leaderless Replication.

```xml
Dynamo-Style: Leaderless Architecture 
Cassandra: Leaderless Architecture
Amazon DynamoDB: Single-Leader Architecture.
```
### Issues in leaderless replication and methods to resolve it:
#### Read Repair:
When a client makes a read from several nodes in parallel, it can detect any stale responses using version numbers. When a client sees that a replica has a stale value, it writes the newer value back to that replica. 
#### Anti-Entropy Process:
In addition to Read Repair, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. This process is called Anti-entropy process.
#### Hinted handoff:
when a node is unreachable, another node can accept writes on its behalf. The write is then kept in a local buffer and sent out once the destination node is reachable again. This makes Dynamo ‚Äúalways writeable.‚Äù Thus, even in the extreme case where only a single node is alive, write requests will still get accepted and eventually processed.


### What's Next?
Here are some list of topics you might be interested to read further to better understand about replication:

- How to handle different node outages?
- When to use single-leader replication strategy?
- When to use leaderless replication strategy?
- What are some problems with replication lags?
- How to detect and handle write conflicts in distributed system?
