# Distributed Database Concepts (Cassandra-style)

## 1. Merkle Tree
What?
A Merkle tree is a binary hash tree used to efficiently compare large datasets between replicas.

Why?
To detect and fix inconsistencies between replicas without transferring entire datasets.

Use case:
During anti-entropy repair or read repair, nodes exchange Merkle tree root hashes. If roots differ, they compare subtrees until they find differences and sync only those parts.

Example:
Two nodes have replica data but some keys diverged. They build Merkle trees over their data ranges. By comparing hashes top-down, they efficiently find exactly which keys differ.

## 2. Hinted Handoff
What?
A technique to improve availability during node failures.

Why?
If a replica node is down during a write, the coordinator stores a "hint" that it needs to forward that write later when the node recovers.

Example:
Client writes to Cassandra, but Node B is down. Coordinator writes to Node A and stores a hint for Node B. When Node B comes back online, coordinator forwards the missed writes.

## 3. Gossip Protocol
What?
A decentralized, peer-to-peer protocol nodes use to exchange cluster state.

Why?
To discover cluster membership, node health (up/down), token ownership, and metadata updates.

Example:
Node A tells Node B about Node Câ€™s status. Node B shares with Node D. Eventually, all nodes have consistent views of the cluster without a central authority.

## 4. Leaderless Architecture
What?
Cassandra has no single leader; any node can receive client requests.

Why?
To avoid a single point of failure and enable high availability and horizontal scaling.

Example:
Client connects to any node (coordinator). The coordinator routes requests to replicas, handles consistency, and returns results without a master node.

## 5. Quorum Reads and Writes
What?
Consistency level specifying how many replicas must acknowledge read/write.

Why?
To balance availability and consistency.

Example:
With replication factor = 3:

Write quorum = 2 (must be acknowledged by at least 2 replicas)

Read quorum = 2 (read from at least 2 replicas)

This ensures that read and write sets overlap, maintaining consistency.

## 6. Coordinator Node
What?
The node that receives the client request and coordinates the read/write with replicas.

How does it route requests?

Hashes the partition key â†’ generates a token

Uses token ring metadata (via gossip) to identify which replicas own that token

Sends requests to those replicas based on consistency level

Example:
Client requests read for user:123. Coordinator hashes user:123, finds responsible nodes, sends requests, and returns combined result.

### How Does the Coordinator Know Which Shard (Replica Node) to Use in Cassandra?
<details>

âœ… Step-by-Step:
### 1. Hash the Partition Key (Using Partitioner)
Cassandra uses a partitioner (usually Murmur3Partitioner) to hash the partition key (e.g., user:123), which results in a token (a number in a ring).

```go
token = hash(partition_key)
```

### 2. Map Token to Responsible Node(s)
Each Cassandra node is assigned a range of tokens. Cassandra arranges nodes in a ring.

The coordinator:

Uses the token from the hash

Finds the first node clockwise on the ring that owns that token range (called the primary replica)

Then finds other replicas (based on replication factor and replica placement strategy)

### 3. Send Requests to Replica Nodes
- Depending on whether itâ€™s a read or write, and the desired consistency level (e.g., ONE, QUORUM, ALL), the coordinator will:
- Send the request to the appropriate set of replica nodes
- Wait for the required number of acknowledgments before returning

### Where Does the Coordinator Get This Info?
The coordinator maintains metadata called the token map, which it receives via the Gossip protocol. This includes:

- All nodes in the ring
- Their token ranges
- Which nodes are up or down
- This metadata is kept in memory and updated frequently.
- ðŸ”„ If Cluster Changes?
- When nodes are added/removed or token ownership changes:
- Gossip + system.peers table update the metadata
- The coordinator nodes update their local token maps accordingly

</details>

## 7. Partitioning and Consistent Hashing
What?
Data is distributed across nodes via hashing keys into a ring of tokens.

Why?
To distribute data evenly and scale horizontally.

Example:
Key user:123 hashes to token 92345678. The node responsible for that token range stores the data. Replication ensures copies on other nodes.

Putting It All Together: Example Scenario
Client writes data to any Cassandra node (coordinator).

Coordinator hashes key â†’ finds replicas using gossip info.

Coordinator attempts to write to all replicas.

If a replica is down â†’ hinted handoff stores a hint to replay later.

For reads, coordinator queries enough replicas to satisfy quorum.

If replicas differ, read repair and Merkle trees help detect and fix inconsistencies.

Nodes continuously exchange cluster state using gossip.

No leader means no single failure point; any node can be a coordinator.
